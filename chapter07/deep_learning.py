# -*- coding: utf-8 -*-
"""deep_learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OG-0JuPFooeZ6xvyRlsgKSGwUdJpY9FC
"""

# 인공 신경망
from tensorflow import keras
# 패션 MNIST 데이터셋 은 가장 유명한 딥러닝 데이터셋
(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()

print(train_input.shape, train_target.shape)
print(test_input.shape, test_target.shape)

import matplotlib.pyplot as plt
fig, axs = plt.subplots(1, 10, figsize=(10,10))
for i in range(10):
  axs[i].imshow(train_input[i], cmap='gray_r')
  axs[i].axis('off')
plt.show()

print([train_target[i] for i in range(10)])

import numpy as np
print(np.unique(train_target, return_counts=True))

# 로지스틱 회귀로 패션 아이템 분류
# 샘플이 많으므로, 확률적 경사 하강법 사용 (점진적)
train_scaled = train_input / 255.0
train_scaled = train_scaled.reshape(-1, 28*28)
print(train_scaled.shape)

from sklearn.model_selection import cross_validate
from sklearn.linear_model import SGDClassifier
sc = SGDClassifier(loss='log', max_iter=5, random_state=42)
scores = cross_validate(sc, train_scaled, train_target, n_jobs=-1)
print(np.mean(scores['test_score']))

# 인공 신경망
# 가장 기본적인 인공 신경망은 확률적 경사 하강법을 사용하는 로지스틱 회귀와 같다.

# 로지스틱 회귀 공식은 z = a(가중치)x(특성1),,+b(절편)
# 784개(픽셀)의 특성 즉,  z = ax(픽셀1),,,a784x784(픽셀784) +b
# 뉴런(neuron)은 z값을 계산하는 단위 = unit 이라고도 표현

# 인공 신경망은 우리 뇌에 있는 뉴런과 같지 않다. 정말 뇌 속에 있는 무언가를 만드는 일이 아니라는 것을 꼭 기억
# 인공 신경망은 기존의 머신러닝 알고리즘이 잘 해결하지 못했던 문제에서 높은 성능을 발휘하는 새로운 종류의 머신러닝 알고리즘

# 딥러닝은 인공 신경망과 거의 동의어 혹은 심층 신경망(deep neural network, DNN)

# 텐서플로와 케라스
import tensorflow as tf
# tensorflow는 저수준 API와 고수준 API가 있다. 
# Keras가 고수준 API : 딥러닝 라이브러리이고 머신러닝 라이브러리와 차이점은 GPU(그래픽 처리 장치)를 사용하여 인공 신경망을 훈련
# GPU는 백터와 행렬 연산에 매우 최적화되어 있다.

# 현재는 텐서플로와 케라스는 거의 동의어이다.
# 케라스 라이브러리는 직접 GPU 연산을 수행하지 않는다. 대신 GPU 연산을 수행하는 다른 라이브러리를 백엔드로 사용한다. 예를 들면, 텐서플로가 케라스의 백엔드 중 하나다.
# 텐서플로 라이브러리에 케라스 API가 내장되고, 케라스를 제외한 나머지 고수준 API를 모두 없앴다.

from tensorflow import keras

# 인공 신경망으로 모델 만들기
from sklearn.model_selection import train_test_split
train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42)

print(train_scaled.shape, train_target.shape)
print(val_scaled.shape, val_target.shape)

#Dense(뉴런개수, activation=뉴런의 출력에 적용할 함수, input_shape=입력의 크기(각 뉴련이 몇개의 입력을 받는지))
dense = keras.layers.Dense(10, activation='softmax', input_shape=(784,))

# 여기서 만든 model 객체가 신경망 모델
model = keras.Sequential(dense)

# 절편이 뉴런마다 더해진다는 것을 꼭 기억
# 소프트맥스와 같이 뉴런의 선형 방정식 계산 결과에 적용되는 함수를 활성화 함수(activation function)이라고 부름.

# 인공 신경망으로 패션 아이템 분류하기

# 케라스 모델은 훈련하기 전에 설정 단계가 있음 conpile() - 꼭 지정해야 할 것은 손실 함수의 종류
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')

# 이진 분류일때 사용하는 손실 함수 : loss='binary_crossentropy' 
# 다중 분류일 떄 사용하는 손실 함수 : loss='categorical_crossentropy'
# 원-핫 인코딩(one-hot encoding) : 타깃값을 해당 클래스(활성화 출력)만 1이고 나머지는 모두 0인 배열로 만드는 것을 의미. 
# 따라서 다중(이진) 분류에서 크로스 엔트로피 손실 함수를 사용하려면 정수로 된 타깃값을 one-hot encoding으로 변환해야한다.
# 그래서 정수로된 타깃값으로 크로스 엔트로피 손실을 계산하는 것이 sparse_..._crossentropy 이다.
# target 값을 one-hot encoding으로 준비했다면 categorical_crossentropy 사용 가능.

# 에포크 횟수 = epochs
model.fit(train_scaled, train_target, epochs=5)

print("-----------------------------------------------------------------------------------\n")
# 모델 성능 평가
model.evaluate(val_scaled, val_target)

# 심층 신경망
from tensorflow import keras
(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()

from sklearn.model_selection import train_test_split
train_scaled = train_input / 255.0
train_scaled = train_scaled.reshape(-1, 28*28)
train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42)
# 분류 문제는 클래스에 대한 확률을 출력하기 위해 활성화 함수를 사용한다. 회귀의 출력은 임의의 어떤 숫자이므로 활성화 함수를 적용할 필요가 없다.
# 모든 신경망의 은닉층에는 항상 활성화 함수가 있다.

# 몇 개의 뉴런을 두어야 할지 판단하기 위해서는 경험이 필요.
# dnese1 = 은닉층, dense2 = 출력층 : 출력층보다는 뉴런을 더 많이 두어야한다.**
dense1 = keras.layers.Dense(100, activation='sigmoid', input_shape=(784,))
dense2 = keras.layers.Dense(10, activation='softmax')

# 심층 신경망 만들기
# 주의할 점 : 가장 처음 등장하는 은닉층을 처음에 두고 가장 나중에 등장하는 출력층을 나중에 두어야한다.
model = keras.Sequential([dense1, dense2])
# 샘플의 개수가 None인 이유 : 미니배치 경사 하강법을 사용하므로, 즉, 한 번에 다 훈련하지 않는다.
model.summary()

# 층을 추가하는 다른 방법
# space 바를 사용해서 이름을 만들면.. 이 이름을 인식못한다. ex) fashion MNIST model => error
model = keras.Sequential([keras.layers.Dense(100, activation='sigmoid', input_shape=(784,),name='hidden'),keras.layers.Dense(10, activation='softmax', name='output')], name='fashionMNISTModel')

model.summary()

# add 메서드 사용할 것.
model = keras.Sequential(name='fashionModel')
model.add(keras.layers.Dense(100, activation='sigmoid', input_shape=(784,)))
model.add(keras.layers.Dense(10, activation='softmax'))

model.summary()

# train
# 몇 개의 층을 추가하더라도 compile과 fit 사용법은 동일.
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')
model.fit(train_scaled, train_target, epochs=5)

# 시그모이드 함수 
# 초창기 인공 신경망에서 많이 사용된 활성화 함수는 시그모이드 함수. 하지만 이 함수의 단점은 오른쪽과 왼쪽 끝으로 갈수록 그래프가 누워있기 때문에 올바른 출력을 만드는데 신속하게 대응 못함.
# 층이 많은 심층 신경망일 수록 학습 힘듬

# 렐루 함수(RelU)
# 그래서 나온것이 렐루 함수.
# 입력이 양수일 경우 마치 활성화 함수가 없는 것처럼 그냥 입력을 통과시키고 음수일 경우에는 0으로 만든다.
# 심층 신경망에서 뛰어남.
# max(0, z)

# numpy 배열의 reshape 함수 역할을 Flatten 제공 (인공 신경망 성능에 기여하는 것이 아니라 입력 차원을 일렬로 펼쳐줌)
model = keras.Sequential()
model.add(keras.layers.Flatten(input_shape=(28, 28)))
model.add(keras.layers.Dense(100, activation='relu'))
model.add(keras.layers.Dense(10, activation='softmax'))
# 3개 추가했지만 깊이는 2이다. Flatten은 학습하는 층이 아니므로
model.summary()

# 입력 데이터에 대한 전처리 과정을 가능한 모델에 포함시키는 것이 케라스 API의 철학

(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()
train_scaled = train_input / 255.0
train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42)

model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')
model.fit(train_scaled, train_target, epochs=5)

print("--------------------------------------------------------------------------\n")

model.evaluate(val_scaled, val_target)

# 옵티마이저(optimizer)
# 케라스가 제공하는 다양한 종류의 경사 하강법 알고리즘을 옵티마이저라고 부름

# 'sgd' 문자열을 보내면 아래 SGD 객체를 자동으로 만들어준다.
model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics='accuracy')

s = keras.optimizers.SGD()
model.compile(optimizer=s, loss='sparse_categorical_crossentropy', metrics='accuracy')

# sgd 말고도 다양한 옵티마이저들이 있음.

# momentum 매개변수의 default =0 0보다 큰값으로 사용하면 이전의 그레이디언트를 가속도처럼 사용하는 모멘텀 최적화(momentum optimization)를 사용.
# 보통 momentum 매개변수는 0.9 이상 지정
# SGD class의 nesterov 매개변수를 True로 바꾸면 네스테로프 모멘텀 최적화(nesterov momentum optimization)을 사용
sgd = keras.optimizers.SGD(momentum=0.9, nesterov=True)

# 적응적 학습률(adaptive learning rate)
# 모델이 최적점에 가까이 갈수록 학습률을 낮출 수 있다.
# 적응적 학습률을 사용하는 대표적인 옵티마이저는 Adagrad, RMSprop
adagrad = keras.optimizers.Adagrad()
model.compile(optimizer=adagrad, loss='sparse_categorical_crossentropy', metrics='accuracy')
rmsprop = keras.optimizers.RMSprop()
model.compile(optimizer=rmsprop, loss='sparse_categorical_crossentropy', metrics='accuracy')

model = keras.Sequential()
model.add(keras.layers.Flatten(input_shape=(28,28)))
model.add(keras.layers.Dense(100, activation='relu'))
model.add(keras.layers.Dense(10, activation='softmax'))
# 모멘텀 최적화와 RMSprop 장점을 접목한 것이 Adam이다.
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')
model.fit(train_scaled, train_target, epochs=5)

print("-------------------------------------------------------------------------------------\n")
model.evaluate(val_scaled, val_target)

# 신경망 모델 훈련

# 머신러닝 알고리즘 특징 : 모델의 구조가 어느 정도 고정되어 있다고 느낄 수 있다.
# 반면에 딥러닝에서는 모델의 구조를 직접 만든다는 느낌이 훨씬 강하다.
# 층을 추가하고 층에 있는 뉴런의 개수와 활성화 함수를 결정하는 일들이 그렇다.
# 그래서 딥러닝 분야에서는 연구자와 프로그래머가 더 밀접하게 일하게 되는 것 같다고 함.

# 여기부터 케라스 API를 사용해 모델을 훈련하는데 필요한 다양한 도구들을 사용해보자.

# 손실 곡선
from tensorflow import keras
from sklearn.model_selection import train_test_split
(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()
train_scaled = train_input / 255.0
train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42)

# func : make a model 
def model_fn(a_layer=None):
  model = keras.Sequential()
  model.add(keras.layers.Flatten(input_shape=(28,28)))
  model.add(keras.layers.Dense(100, activation='relu'))
  if a_layer:
    model.add(a_layer)
  model.add(keras.layers.Dense(10, activation='softmax'))
  return model

model = model_fn()
model.summary()

model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')
# verbose 는 훈련 과정 출력을 조절 : defualt = 1(막대포함), 2(막대x), 0(출력x)
history = model.fit(train_scaled, train_target, epochs=5, verbose=0)

# 훈련 측정 값이 담겨 있음 history 메서드에
print(history.history.keys())

# loss graph
import matplotlib.pyplot as plt
plt.plot(history.history['loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()

# accuracy graph
plt.plot(history.history['accuracy'])
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()

# epoch 횟수가 증가할 수록 loss가 낮아지고 accuracy가 높아지므로 epochs=20
model = model_fn()
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')
history = model.fit(train_scaled, train_target, epochs=20, verbose=0)

plt.plot(history.history['loss'])
plt.xlabel('epoch20')
plt.ylabel('loss')
plt.show()

plt.plot(history.history['accuracy'])
plt.xlabel('epoch20')
plt.ylabel('accuracy')
plt.show()

# 검증 손실
# 인공 신경망 모델이 최적화하는 대상은 정확도가 아니라 손실 함수이다. 모델이 잘 훈련되었는지 판단하려면 손실 함수의 값을 확인하는 것이 더 낫다.
model = model_fn()
model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')
history = model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target))
print(history.history.keys())

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'])
plt.show()

# 훈련 손실은 꾸준히 감소해서 전형적인 과대적합 모델이 만들어진다. 전형적인 과대적합 모델이 만들어진다.
# 검증 손실이 상승하는 시점을 가능한 뒤로 늦추면 검증 세트에 대한 손실이 줄어들 뿐만 아니라 검증 세트에 대한 정확도도 증가함.
# 즉 아래 그래프에서는 epoch=5 시점..

# 이걸 해결하기 위해 규제 방법 or 옵티마이저 하이퍼파라미터 조정 방법
model = model_fn()
# 여기선 옵티마이저를 적용함.
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')
history = model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'])
plt.show()
# 과대 적합이 훨씬 줄었다.
# 더 나은 손실 곡선을 얻으려면 학습률을 조정해서 얻을 수도 있다.

# 드룹아웃(dropout)
# : 훈련 과정에서 층에 있는 일부 뉴런을 랜덤하게 꺼서( 즉, 뉴런의 출력을 0으로 만들어) 과대적합을 막는다.
model = model_fn(keras.layers.Dropout(0.3))
model.summary()
# 은닉층 뒤에 추가된 드롭아웃 층은 훈련되는 모델 파라미터가 없다. 또한 입력과 출력의 크기가 같다. 일부 뉴런의 출력을 0으로 만들지만 전체 출력 배열의 크기를 바꾸지는 않는다.

# 물론 훈련이 끝난 뒤에 평가나 예측을 수행할 때는 드룹아웃을 적용하지 말아야 한다. 훈련된 모든 뉴런을 사용해야 올바른 예측을 수행할 수 있다. 
# 훈련이 끝난 뒤에 평가나 예측에는 텐서플로 케라스에서 자동으로 제외시켜준다.**

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')
history = model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target))
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'])
plt.show()

# 모델 저장과 복원
model = model_fn(keras.layers.Dropout(0.3))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')
history = model.fit(train_scaled, train_target, epochs=10, verbose=0, validation_data=(val_scaled, val_target))

# 훈련된 모델 파라미터를 저장함.
# default는 텐서플로의 체크포인트 포맷으로 저장, but 파일의 확장자가 .h5 라면, HDF5 포맷으로 저장
model.save_weights('model-weights.h5')

#default = 텐서플로의 SavedModel 포맷
model.save('model-whole.h5')

model = model_fn(keras.layers.Dropout(0.3))
# 당연한 이야기지만 save_weights 메서드로 저장했던 모델과 같은 구조를 가져야한다. load_weights()
model.load_weights('model-weights.h5')

import numpy as np
# argmax() return 배열에서 가장 큰 값의 index
val_labels = np.argmax(model.predict(val_scaled), axis=-1)
print(np.mean(val_labels == val_target))

# 같은 모델을 저장하고 로드
model = keras.models.load_model('model-whole.h5')
model.evaluate(val_scaled, val_target)

# 모델을 두 번씩 훈련하지 않고 한 번에 끝낼 순 없을까..?
# 케라스의 콜백
# 콜백은 훈련 과정 중간에 어떤 작업을 수행할 수 있게 하는 객체

model = model_fn(keras.layers.Dropout(0.3))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')
checkpoint_cb = keras.callbacks.ModelCheckpoint('best-model.h5')
model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb])

model = keras.models.load_model('best-model.h5')
model.evaluate(val_scaled, val_target)

# 검증 점수가 상승하기 시작하면 그 이후에는 과대적합이 더 커지기 때문에 훈련을 계속할 이유가 없음. 
# 그래서 조기 종료(early stopping)을 함. 즉, 최적의 에포크에서 종료한다는 의미

model = model_fn(keras.layers.Dropout(0.3))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')
checkpoint_cb = keras.callbacks.ModelCheckpoint('best-model.h5')
early_stopping_cb = keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)
history = model.fit(train_scaled, train_target, epochs=20, verbose=0, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb, early_stopping_cb])

# 즉, stpped_epoch 에서 가장 낮은 손실이 나왔다는 것을 의미.
print(early_stopping_cb.stopped_epoch)

# 훈련 손실, 검증 손실
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'])
plt.show()

model.evaluate(val_scaled, val_target)

# ModelCheckpoint 콜백과 함께 사용하면 최상의 모델을 자동으로 저장해 주므로 편리함.

# 정리
# dropout : 과대적합을 막기 위해 신경망에서 자주 사용되는 대표적인 규제 방법