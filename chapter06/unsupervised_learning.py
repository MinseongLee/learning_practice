# -*- coding: utf-8 -*-
"""unsupervised_learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kh0fwpyeiPYq3Q9oBOA_LeRPY4V3Mfev
"""

# 비지도 학습(unsupervised learning)
# 타깃 없이 학습

# 사과, 파인애플, 바나나 사진임을 미리 알고 구했으므로,, 진짜 비지도 학습이라고 부르기 힘듬.
import numpy as np
import matplotlib.pyplot as plt

fruits = np.load('fruits_300.npy')
# 3차원 배열
print(fruits.shape)
# 픽셀 100개에 들어 있는 값 출력, 흑백 사진이므로 0~255까지의 정수값
print(fruits[0,0,:])

plt.imshow(fruits[0], cmap='gray')
plt.show()

# 약간 보기 좋게..
# 밝은 부분은 0에 가깝고, 짙은 부분은 255에 가깝다.
plt.imshow(fruits[0], cmap='gray_r')
plt.show()

# 여러 개의 그래프를 배열처럼(row, column)
fig, axs = plt.subplots(1, 2)
axs[0].imshow(fruits[100], cmap='gray_r')
axs[1].imshow(fruits[200], cmap='gray_r')
plt.show()

# 픽셀값 분석(전처리)
# 2차원 => 1차원
apple = fruits[0:100].reshape(-1, 100*100)
pineapple = fruits[100:200].reshape(-1, 100*100)
banana = fruits[200:300].reshape(-1, 100*100)

print(pineapple.shape)

# axis=0 : row 방향 계산, axis=1 : column 방향 계산
# print(apple.mean(axis=1))

# column 값 계산한 평균으로 그린 그래프
plt.hist(np.mean(apple, axis=1), alpha=0.8)
plt.hist(np.mean(pineapple, axis=1), alpha=0.8)
plt.hist(np.mean(banana, axis=1), alpha=0.8)
plt.legend(['apple', 'pineapple', 'banana'])
plt.show()

# row 값 평균으로 그린 그래프
fig, axs = plt.subplots(1, 3, figsize=(20, 5))
axs[0].bar(range(10000), np.mean(apple, axis=0))
axs[1].bar(range(10000), np.mean(pineapple, axis=0))
axs[2].bar(range(10000), np.mean(banana, axis=0))
plt.show()

# 평균값을 100 x 100 크기로
apple_mean = np.mean(apple, axis=0).reshape(100, 100)
pineapple_mean = np.mean(pineapple, axis=0).reshape(100, 100)
banana_mean = np.mean(banana, axis=0).reshape(100, 100)
fig, axs = plt.subplots(1, 3, figsize=(20, 5))
axs[0].imshow(apple_mean, cmap='gray_r')
axs[1].imshow(pineapple_mean, cmap='gray_r')
axs[2].imshow(banana_mean, cmap='gray_r')
plt.show()

# 평균값과 가까운 사진 고르기
abs_diff = np.abs(fruits - apple_mean)
# 각 샘플에 대한 평균을 구하기 위해 두번째, 세번째 차원을 모두 더하기. => 1차원에
abs_mean = np.mean(abs_diff, axis=(1,2))
# 일차원 배열(모든 합계)
print(abs_mean.shape)
# 가장 작은 순서대로 100개
apple_index = np.argsort(abs_mean)[:100]
# apple_mean과 오차가 가장 작은 샘플 100개 고르기.
fig, axs = plt.subplots(10, 10, figsize=(10,10))
for i in range(10):
  for j in range(10):
    axs[i, j].imshow(fruits[apple_index[i*10 + j]], cmap='gray_r')
    axs[i, j].axis('off')
plt.show()

# 이렇게 비슷한 샘플끼리 그룹으로 모으는 작업을 군집(clustering)이라고 함.
# 그룹을 cluster라고 함.

# k-평균 알고리즘
# k-means(k-평균) 군집 알고리즘이 평균값을 자동으로 찾아준다.
# 작동 방식
# 1. 무작위로 k개의 클러스터 중심을 정함
# 2. 각 샘플에서 가장 가까운 클러스터 중심을 찾아 해당 클러스터의 샘플로 지정
# 3. 클러스터에 속한 샘플의 평균값으로 클러스터 중심을 변경
# 4. 클러스터 중심에 변화가 없을 때까지 2번으로 돌아가 반복

import numpy as np
fruits = np.load('fruits_300.npy')
# (sample cnt, weight, height) => (sample cnt, weight * height)
fruits_2d = fruits.reshape(-1, 100*100)

from sklearn.cluster import KMeans
km = KMeans(n_clusters=3, random_state=42)
# 비지도학습이므로, target이 없다.
km.fit(fruits_2d)
# 군집된 결과는 labels_ 에 저장
# n_clusters=3이므로, 0,1,2 중 나온다.
print(km.labels_)
# 0 = cnt(91), 1 = cnt(98), 2 = cnt(111)
print(np.unique(km.labels_, return_counts=True))

import matplotlib.pyplot as plt
# arr[sample cnt, weight, height]
def draw_fruits(arr, ratio=1):
  # cnt sample
  n = len(arr)
  # 한 줄에 10개씩
  rows = int(np.ceil(n/10))
  cols = n if rows < 2 else 10
  fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False)
  for i in range(rows):
    for j in range(cols):
      if i*10 + j < n:
        axs[i, j].imshow(arr[i*10+j], cmap='gray_r')
      axs[i, j].axis('off')
  plt.show()
# km.labels_ 배열에서 값이 0인 위치는 True else False : boolean indexing
draw_fruits(fruits[km.labels_==0])
draw_fruits(fruits[km.labels_==1])
draw_fruits(fruits[km.labels_==2])

# 클러스터 중심
# KMeans 클래스가 최종적으로 찾은 클러스터 중심은 cluster_centers_ 속성에 저장
draw_fruits(km.cluster_centers_.reshape(-1, 100, 100), ratio=3)

# index가 100인 샘플 fruits_2d[100] = 이때 항상 2차원배열을 줘야한다.
print(km.transform(fruits_2d[100:101]))
# predict() 가장 가까운 클러스터 중심을 예측 클래스로
print(km.predict(fruits_2d[100:101]))

# 위에서 파인애플로 예측결과가 나온것과 동일.
draw_fruits(fruits[100:101])

# k-평균 알고리즘은 반복적으로 클러스터 중심을 옮기면서 최적의 클러스터를 찾는다.
# n_iter : 반복횟수
print(km.n_iter_)

# 실전에서는 클러스터 개수조차 알 수 없다. 최적의 k를 어떻게 찾을까?

# 최적의 k 찾기
# k-평균 알고리즘의 단점 중 하나는 클러스터 개수를 사전에 지정해야한다는것
# 적절한 k 값을 찾는 완벽한 방법은 없다. 여러 방법 중 하나인 elbow 방법에 대해 알아봄
# inertia(이너셔) : (클러스터 중심과 클러스터에 속한 샘플 사이의 거리)의 제곱의 합을 의미
inertia = []
# 클러스터 개수를 증가시키면서 이너셔가 크게 줄어들지 않는 지점을 찾기 위함.
for k in range(2, 10):
  km = KMeans(n_clusters=k, random_state=42)
  km.fit(fruits_2d)
  inertia.append(km.inertia_)
plt.plot(range(2, 10),inertia)
plt.xlabel('k')
plt.ylabel('inertia')
plt.show()
# 명확하진 않지만 꺽이는 지점이 3,,, 8일때도 꺽이지만 클러스터 개수를 늘릴수록 계속 줄어드네..이건 그러므로.. 이경우에는 엘보우 방법이 좋지 못하다고 생각

# 주성분 분석(principal component analysis) : 대표적 차원 축소 알고리즘
# 차원 축소(dimensionality reduction) : 데이터를 가장 잘 나타내는 일부 특성을 선택하여 데이터 크기를 줄이고 지도 학습 모델 성능 향상
# 분산이 큰 방향을 찾는것이 중요. 이 직선이 원점에서 출발한다면 두 원소로 이루어진 벡터로 사용가능.
# 이 벡터를 주성분이라고 부름 : 이 주성분 벡터는 원본 데이터에 있는 어떤 방향
# 즉, 주성분 벡터 원소 개수 == 원본 데이터 특성 개수
# 주성분은 원본 차원과 같고 주성분으로 바꾼 데이터는 차원이 줄어든다.**

import numpy as np
fruits = np.load('fruits_300.npy')
fruits_2d = fruits.reshape(-1, 100*100)
# 주성분 개수 = n_components
from sklearn.decomposition import PCA
pca = PCA(n_components=50)
pca.fit(fruits_2d)

print(pca.components_.shape)

draw_fruits(pca.components_.reshape(-1, 100, 100))

print(fruits_2d.shape)
fruits_pca = pca.transform(fruits_2d)
# pca 주성분을 50으로 지정했으므로, 10,000 => 50으로.
print(fruits_pca.shape)

# 원본 데이터 재구성
# 손실 발생 가능성 존재 but 대부분 복구 가능.
fruits_inverse = pca.inverse_transform(fruits_pca)
print(fruits_inverse.shape)

fruits_reconstruct = fruits_inverse.reshape(-1, 100, 100)
for start in [0, 100, 200]:
  draw_fruits(fruits_reconstruct[start:start+100])
  print("\n")

# 설명된 분산(explained variance) : 주성분이 원본 데이터의 분산을 얼마나 잘 나타내는지 기록한 값

print(np.sum(pca.explained_variance_ratio_))
plt.plot(pca.explained_variance_ratio_)
plt.xlabel('principal component')
plt.show()

# 다른 알고리즘과 함께 사용하기
# 로지스틱 회귀 모델
from sklearn.linear_model import LogisticRegression

lr = LogisticRegression()

target = np.array([0]*100 + [1]*100 + [2]*100)

from sklearn.model_selection import cross_validate
# 2d 사용
scores = cross_validate(lr, fruits_2d, target)
print(np.mean(scores['test_score']))
print(np.mean(scores['fit_time']))

#fruits_pca 사용
# 즉, pca로 훈련 데이터 차원을 축소하면 저장 공간뿐만 아니라 머신러닝 모델의 훈련 속도도 높일 수 있다.
scores = cross_validate(lr, fruits_pca, target)
print(np.mean(scores['test_score']))
print(np.mean(scores['fit_time']))

pca = PCA(n_components=0.5)
pca.fit(fruits_2d)
# 특성이 2개.
print(pca.n_components_)

fruits_pca2 = pca.transform(fruits_2d)
print(fruits_pca2.shape)

scores = cross_validate(lr, fruits_pca2, target)
print(np.mean(scores['test_score']))
print(np.mean(scores['fit_time']))

# with k-평균 알고리즘 using data of 차원 축소
from sklearn.cluster import KMeans
km = KMeans(n_clusters=3, random_state=42)
# 축소된 데이터 사용하는 것이 원본 데이터를 사용하는 것과 거의 비슷한 결과 
km.fit(fruits_pca2)
print(np.unique(km.labels_, return_counts=True))

for label in range(0, 3):
  draw_fruits(fruits[km.labels_ == label])
  print("\n")

# 훈련 데이터의 차원을 줄였을 때 장점 중 하나는 시각화이다. 
for label in range(0, 3):
  data = fruits_pca2[km.labels_ == label]
  plt.scatter(data[:,0], data[:,1])
plt.legend(['apple', 'banana', 'pineapple'])
plt.show()

# 차원 축소를 사용하면 데이터셋의 크기를 줄일 수 있고 시각화하기 쉬움
# 차원 축소된 데이터를 지도 학습 알고리즘이나 다른 비지도 학습 알고리즘에 재사용하여 성능을 높이거나 훈련 속도를 빠르게 만들 수 있음