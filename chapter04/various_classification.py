# -*- coding: utf-8 -*-
"""various_classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZdjxVaourIKbXu1vg9iNLSBIy2HmADa1
"""

import pandas as pd

fish = pd.read_csv('https://bit.ly/fish_csv_data')
print(fish.head())

print(pd.unique(fish['Species']))

fish_input = fish[['Weight', 'Length', 'Diagonal', 'Height', 'Width']].to_numpy()
print(fish_input[:5])

fish_target = fish['Species'].to_numpy()

from sklearn.model_selection import train_test_split
# 이렇게 input과 target이 만들어졌으면 need train set and test set 
train_input, test_input, train_target, test_target =  train_test_split(fish_input, fish_target, random_state=42)

# preprocessing
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

# predict the value
from sklearn.neighbors import KNeighborsClassifier
kn = KNeighborsClassifier(n_neighbors=3)
kn.fit(train_scaled, train_target)
print(kn.score(train_scaled,train_target))
print(kn.score(test_scaled, test_target))

# 다중분류
# : target에 3개 이상의 클래스가 포함된 문제를 다중 분류(multi-class classification)
# 이진분류 - 2개..
# 저장순서와 상관없이 알파벳 순서로 정렬
print(kn.classes_)

print(kn.predict(test_scaled[:5]))

import numpy as np
# class별 확률값을 반환
proba = kn.predict_proba(test_scaled[:5])
# 즉, 클래스가 7개란 소리
print(np.round(proba, decimals=4))

distances, indexes = kn.kneighbors(test_scaled[3:4])
print(train_target[indexes])

# 로지스틱 회귀(logistic regression)
# 이름은 회귀지만 분류 모델이다.
# 선형 방정식을 학습한다. 이 때, z = ax + by + cm + dn + e 일때,, z는 어떤 값이든 가능하지만 확률이려면 0~1이어야한다.(0~100%)
# 로지스틱함수(logistic function) = 시그모이드 함수(sigmoid function)를 사용하면 가능
# f(x)= 1/1+e^-x
import numpy as np
import matplotlib.pyplot as plt
z = np.arange(-5, 5, 0.1)
phi = 1 / (1 + np.exp(-z))
plt.plot(z, phi)
plt.xlabel('z')
plt.ylabel('phi')
plt.show()

#이진 분류일 경우 (0.5 < x, 양성), (0.5 >= x, 음성) 이다
bream_smelt_indexes = (train_target == 'Bream') | (train_target == 'Smelt')
train_bream_smelt = train_scaled[bream_smelt_indexes]
target_bream_smelt = train_target[bream_smelt_indexes]

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
lr.fit(train_bream_smelt,target_bream_smelt)

print(lr.predict(train_bream_smelt[:5]))
# 예측 확률
print(lr.predict_proba(train_bream_smelt[:5]))

# index로 보면 0, 1 이므로, Smelt가 양성. 만약 Bream을 양성으로 놓고 싶다면 이걸 target=1로 지정해주면된다.
print(lr.classes_)

# 로지스틱 회귀가 학습한 계수
# z = -0.404(Weight)-0.576(Length)-0.663(Diagonal)-1.013(Height)-0.732(Width)-2.16
print(lr.coef_, lr.intercept_)

#decision_function() : z값 출력
decisions = lr.decision_function(train_bream_smelt[:5])
print(decisions)

# 시그모이드 함수
from scipy.special import expit
# 양성 클래스에 대한 z값을 return
print(expit(decisions))

# 로지스틱 회귀로 다중 분류 수행하기
# 반복적인 알고리즘 사용 - max_iter 개수 늘리기. C is like alpha : alpha와 반대로 작을수록 규제가 커짐.
lr = LogisticRegression(C=20, max_iter=1000)
lr.fit(train_scaled, train_target)
print(lr.score(train_scaled,train_target))
print(lr.score(test_scaled, test_target))
print(lr.predict(test_scaled[:5]))
proba = lr.predict_proba(test_scaled[:5])
print(np.round(proba, decimals=3))

print(lr.classes_)

# 행이 7개라는 말은.. z값이 7개란의미. 즉, 클래스 마다 z값을 하나씩 계산한다. 당연히 가장 높은 z 값을 출력하는 클래스
# 이진 분류는 시그모이드 함수, 다중 분류는 소프트맥스(softmax) 함수를 사용하여 7개의 z 값을 확률로 변환한다.
#5는 특성이다.
print(lr.coef_.shape, lr.intercept_.shape)

# softmax func은 여러 개의 선형 방정식의 출력값을 0~1 사이로 압축하고 전체 합이 1이 되도록 만든다.
# softmax 계산 방식
# e_sum = e^z1 +e^z2...+e^z7
# s1 = e^z1/e_sum,,,s7 = e^z7/e_sum
decision = lr.decision_function(test_scaled[:5])
# z1~z7 까지의 softmax func 값
print(np.round(decision, decimals=2))

from scipy.special import softmax
# axis=1을 주면 각 행, 즉 각 샘플에 대해 소프트맥스를 계산
proba = softmax(decision, axis=1)
print(np.round(proba, decimals=3))

# 확률적 경사 하강법(Stochastic Gradient Descent) : 대표적인 점진적 학습 알고리즘
# 최초로 훈련한 데이터를 버리지 않고 새로운 데이터에 대해서만 조금씩 더 훈련하는 방식을 점진적 학습 or 온라인 학습이라고 부름
# 훈련 세트에서 랜덤하게 하나의 샘플을 선택하여 훈련하고 또 다른 샘플 선택.. 이렇게 전체 샘플을 다 사용할 때까지 계속
# 에포크(epoch)는 훈련 세트를 한 번 모두 다 사용하는 것을 의미. 일반적으로 경사 하강법은 수십 수백 번 에포크를 수행

# 손실 함수(loss function) : 머신러닝 알고리즘이 얼마나 엉터리인지를 측정하는 기준 == 비용 함수(cost function)
# 분류에서 손실은 정답을 못 맞추는 것이다.
# 손실 함수를 만들기 위해선 미분 가능해야한다.(즉, 연속적이어야한다.)
# 음성클래스인경우 0이므로, 양성클래스인 경우의 확률로 곱한다. ex) 0.2, 0 => 0.8, 1 => -0.8

# SGDClassifier
import pandas as pd
fish = pd.read_csv('https://bit.ly/fish_csv_data')
fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
fish_target = fish['Species'].to_numpy()

from sklearn.model_selection import train_test_split
train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)

from sklearn.preprocessing import StandardScaler
ss = StandardScaler()
ss.fit(train_input, train_target)
train_scaled = ss.transform(train_input)
test_scaled = ss.transform(test_input)

from sklearn.linear_model import SGDClassifier
# loss는 손실 함수의 종류 지정 loss='log'는 로지스틱 손실 함수
sc = SGDClassifier(loss='log', max_iter=10, random_state=42)
sc.fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))

# 확률적 경사 하강법은 점진적 학습이 가능.
sc.partial_fit(train_scaled, train_target)
print(sc.score(train_scaled, train_target))
print(sc.score(test_scaled, test_target))

# 에포크와 과대/과소적합
# 확률적 경사 하강법을 사용한 모델은 에포크 횟수에 따라 과소적합이나 과대적합이 될 수 있다.
# 적은 에포크 : 과소적합 모델 가능성 높음
# 많은 에포크 : 과대적합 모델 가능성 높음

import numpy as np
sc = SGDClassifier(loss='log', random_state=42)
train_score = []
test_score = []
classes = np.unique(train_target)
print(classes)
for _ in range(0,300):
  sc.partial_fit(train_scaled, train_target, classes=classes)
  train_score.append(sc.score(train_scaled,train_target))
  test_score.append(sc.score(test_scaled,test_target))

import matplotlib.pyplot as plt
# blue = train
plt.plot(train_score)
plt.plot(test_score)
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.show()

# 위에서 확인한 결과 에포크 횟수는 100정도가 적당
sc = SGDClassifier(loss='log', max_iter=100, tol=None, random_state=42)
sc.fit(train_scaled,train_target)
print(sc.score(train_scaled,train_target))
print(sc.score(test_scaled,test_target))

# loss 기본값은 hinge : 힌지 손실(hinge loss), 서포트 벡터 머신(support vector machine)이라 불리는 또 다른 머신러닝 알고리즘을 위한 손실 함수