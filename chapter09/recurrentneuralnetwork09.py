# -*- coding: utf-8 -*-
"""recurrentNeuralNetwork09.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mkD9KlNk85IDgJ9uw-672LbF-ojTHUWP
"""

# 순환신경망(recurrent neural network)
# 순차데이터(sequential data) : 텍스트나 시계열 데이터(time series data)와 같이 순서에 의미가 있는 데이터를 말한다.

# 완전 연결 신경망이나 합성곱 신경망은 이런 기억 장치가 없다. 하나의 샘플(또는 하나의 배치)을 사용하여 정방향 계산을 수행하고 그 샘플을 재사용하지 않음(즉, 기억하지 않음)
# 피드포워드 신경망(feedforward neural network) : 메모리 x, 데이터의 흐름이 앞으로만 전달

# 순환 신경망 : 완전 연결 신경망과 거의 비슷한데 이전 데이터를 처리할 수 있는 메모리만 추가해줌.
# 순환 신경망은 이전 timestep의 샘플을 기억하지만 타임스텝이 오래될수록 순환되는 정보는 희미해진다.
# timestep : 이전 샘플의 기억을 가지고 있는 샘플을 처리하는 한 단계를 의미.

# cell = 층
# 셀의 출력을 은닉 상태(hidden state)

# hyperbolic tangent 함수인 tanh 함수를 많이 사용 (활성화 함수로)

# 자연어 처리(natural language processing, NLP) : 컴퓨터로 인간 언어 처리. 음성 인식, 기계 번역, 감성 분석 등
# 훈련 데이터를 말뭉치(corpus)

# 텍스트 데이터의 경우 단어를 숫자 데이터로 바꾸는 일반적인 방법은 고유한 정수를 부여
# 공백을 기준으로 분리한 단어를 token

from tensorflow.keras.datasets import imdb
# imdb 리뷰 데이터셋
(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words=500)

print(train_input.shape, test_input.shape)
#첫 번째 리뷰 길이
print(len(train_input[0]))
# 두 번째 리뷰 길이
print(len(train_input[1]))

# 단어가 정수로 변환되어져 있음.
print(train_input[0])

# 리뷰가 긍정인지 부정인지 파악 - 이진분류
print(train_target[:20])

from sklearn.model_selection import train_test_split
train_input, val_input, train_target, val_target = train_test_split(train_input, train_target, test_size=0.2, random_state=42)

import numpy as np
lengths = np.array([len(x) for x in train_input])

print(np.mean(lengths), np.median(lengths))

# 평균이 중간값보다 높은 이유는 오른쪽 맨 끝에 큰 값이 있기 때문
import matplotlib.pyplot as plt
plt.hist(lengths)
plt.xlabel('length')
plt.ylabel('frequency')
plt.show()

# 리뷰들의 길이를 100으로 맞추기 위해 패딩이 필요. 보통 패딩을 나타내는 토큰으로는 0을 사용

from tensorflow.keras.preprocessing.sequence import pad_sequences
train_seq = pad_sequences(train_input, maxlen=100)

print(train_seq.shape)

# 첫번째 샘플
print(train_seq[0])
# 둘의 데이터가 일치, 즉, 원본 샘플 앞부분이 짤렸을거라 짐작
print(train_input[0][-10:])

# pad_sequences() default : 시퀀스의 뒷부분 글자가 더 유용할거라 판단해 앞부분을 자름.

print(train_seq[5])

# 검증 세트의 길이도 100으로 맞춤
val_seq = pad_sequences(val_input, maxlen=100)

# 순환 신경망 만들기
from tensorflow import keras
model = keras.Sequential()
# 100은 길이, 500은 load_data()에서 500개의 단어이고, 이 것을 원-핫 인코딩으로 표현하려면 배열의 길이가 500이어야한다. (모두 0이고 해당하는 값만 1인 표현)
model.add(keras.layers.SimpleRNN(8, input_shape=(100, 500)))
model.add(keras.layers.Dense(1, activation='sigmoid'))

train_oh = keras.utils.to_categorical(train_seq)

# 정수 하나마다 모두 500차원의 배열
print(train_oh.shape)

print(train_oh[0][0][:12])

# 모든 원소의 합이 1인지 확인(one-hot encoding)
print(np.sum(train_oh[0][0]))

val_oh = keras.utils.to_categorical(val_seq)

model.summary()

# 순환 신경망 훈련하기

rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)
model.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])
checkpoint_cb = keras.callbacks.ModelCheckpoint('best-simplernn-model.h5')
early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)
history = model.fit(train_oh, train_target, epochs=100, batch_size=64, validation_data=(val_oh, val_target),callbacks=[checkpoint_cb, early_stopping_cb])

# train loss, val loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train','val'])
plt.show()

# 원 핫 인코딩의 단점은 입력 데이터의 크기가 커진다는 것.
print(train_seq.nbytes, train_oh.nbytes)

# 단어 임베딩(word embedding)
# 각 단어를 고정된 크기의 실수 벡터로 바꾸어 준다.
# 입력 데이터의 크기를 줄일 수 있는 방법 - 정수 데이터를 받는다.

model2 = keras.Sequential()
# Embedding(len(words), 벡터의 크기)
model2.add(keras.layers.Embedding(500,16,input_length=100))
model2.add(keras.layers.SimpleRNN(8))
model2.add(keras.layers.Dense(1, activation='sigmoid'))

# 500 * 16
# 16*8 + 8*8+8
model2.summary()

rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)
model2.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])
checkpoint_cb = keras.callbacks.ModelCheckpoint('best-embedding-model.h5')
# 이렇게 단어 임베딩을 사용하니 정확도와 손실률이 훨씬 좋아졌다.
early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)
history = model2.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target), callbacks=[checkpoint_cb, early_stopping_cb])

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'])
plt.show()

# LSTM과 GRU 셀
# 고급 순환층, 성능이 매우 뛰어남

# LSTM(Long Short-Term Memory) : 단기 기억을 오래 기억하기 위해 고안

# LSTM 신경망 훈련하기
from tensorflow.keras.datasets import imdb
from sklearn.model_selection import train_test_split
(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words=500)
train_input, val_input, train_target, val_target = train_test_split(train_input, train_target, test_size=0.2, random_state=42)

from tensorflow.keras.preprocessing.sequence import pad_sequences
train_seq = pad_sequences(train_input, maxlen=100)
val_seq = pad_sequences(val_input, maxlen=100)

# LSTM 셀 순환층
from tensorflow import keras
model = keras.Sequential()
model.add(keras.layers.Embedding(500, 16, input_length=100))
model.add(keras.layers.LSTM(8))
model.add(keras.layers.Dense(1, activation='sigmoid'))

model.summary()

rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)
model.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])
checkpoint_cb = keras.callbacks.ModelCheckpoint('best-lstm-model.h5')
early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)
history = model.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target), callbacks=[checkpoint_cb, early_stopping_cb])

import matplotlib.pyplot as plt
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train','val'])
plt.show()

# 순환층에 드룹아웃 적용
# dropout : 은닉층에 있는 뉴런의 출력을 랜덤하게 꺼서 과대적합을 막는 기법

model2 = keras.Sequential()
model2.add(keras.layers.Embedding(500, 16, input_length=100))
model2.add(keras.layers.LSTM(8, dropout=0.3))
model2.add(keras.layers.Dense(1, activation='sigmoid'))

rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)
model2.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])
checkpoint_cb = keras.callbacks.ModelCheckpoint('best-dropout-model.h5')
early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)
history = model2.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target), callbacks=[checkpoint_cb, early_stopping_cb])

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train','val'])
plt.show()

# 두 개의 층을 연결하기
model3 = keras.Sequential()
model3.add(keras.layers.Embedding(500, 16, input_length=100))
model3.add(keras.layers.LSTM(8, dropout=0.3, return_sequences=True))
model3.add(keras.layers.LSTM(8, dropout=0.3))
model3.add(keras.layers.Dense(1, activation='sigmoid'))

model3.summary()

rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)
model3.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])
checkpoint_cb = keras.callbacks.ModelCheckpoint('best-2rnn-model.h5')
early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)
history = model3.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target), callbacks=[checkpoint_cb, early_stopping_cb])

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'])
plt.show()

# GRU(Gated Recurrent Unit) 구조
# GRU 셀은 LSTM보다 가중치가 적기 때문에 계산량이 적지만 LSTM 못지 않은 좋은 성능을 내는 것으로 알려져 있음

# GRU 신경망 훈련하기
model4 = keras.Sequential()
model4.add(keras.layers.Embedding(500, 16, input_length=100))
model4.add(keras.layers.GRU(8))
model4.add(keras.layers.Dense(1, activation='sigmoid'))

model4.summary()

rmsprop = keras.optimizers.RMSprop(learning_rate=1e-4)
model4.compile(optimizer=rmsprop, loss='binary_crossentropy', metrics=['accuracy'])
checkpoint_cb = keras.callbacks.ModelCheckpoint('best-gru-model.h5')
early_stopping_cb = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)
history = model4.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target), callbacks=[checkpoint_cb, early_stopping_cb])

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend(['train', 'val'])
plt.show()

test_seq = pad_sequences(test_input, maxlen=100)
rnn_model = keras.models.load_model('best-2rnn-model.h5')
rnn_model.evaluate(test_seq, test_target)